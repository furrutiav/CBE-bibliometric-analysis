{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8f4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03886966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ace618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb48b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "e = bert_model.eval()\n",
    "z = bert_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e610c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"preprocess_docs.xlsx\"\n",
    "df = pd.read_excel(file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71c2c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 238\n",
      "1 137\n",
      "2 156\n",
      "3 250\n",
      "4 163\n",
      "5 212\n",
      "6 125\n",
      "7 124\n",
      "8 195\n",
      "9 180\n",
      "10 165\n",
      "11 116\n",
      "12 182\n",
      "13 165\n",
      "14 234\n",
      "15 90\n",
      "16 132\n",
      "17 381\n",
      "18 152\n",
      "19 143\n",
      "20 204\n",
      "21 237\n",
      "22 184\n",
      "23 160\n",
      "24 202\n",
      "25 216\n",
      "26 207\n",
      "27 140\n",
      "28 126\n",
      "29 328\n",
      "30 161\n",
      "31 200\n",
      "32 148\n",
      "33 196\n",
      "34 162\n",
      "35 90\n",
      "36 293\n",
      "37 235\n",
      "38 135\n",
      "39 150\n",
      "40 211\n",
      "41 140\n",
      "42 197\n",
      "43 114\n",
      "44 161\n",
      "45 173\n",
      "46 172\n",
      "47 166\n",
      "48 209\n",
      "49 143\n",
      "50 175\n",
      "51 160\n",
      "52 128\n",
      "53 160\n",
      "54 185\n",
      "55 123\n",
      "56 193\n",
      "57 222\n",
      "58 230\n",
      "59 175\n",
      "60 188\n",
      "61 166\n",
      "62 201\n",
      "63 209\n",
      "64 170\n",
      "65 176\n",
      "66 198\n",
      "67 244\n",
      "68 178\n",
      "69 181\n",
      "70 139\n",
      "71 170\n",
      "72 180\n",
      "73 130\n",
      "74 194\n",
      "75 202\n",
      "76 130\n",
      "77 189\n",
      "78 160\n",
      "79 172\n",
      "80 206\n",
      "81 112\n",
      "82 248\n",
      "83 310\n",
      "84 120\n",
      "85 141\n",
      "86 205\n",
      "87 166\n",
      "88 201\n",
      "89 129\n",
      "90 265\n",
      "91 116\n",
      "92 113\n",
      "93 194\n",
      "94 161\n",
      "95 140\n",
      "96 230\n",
      "97 202\n",
      "98 109\n",
      "99 136\n",
      "100 223\n",
      "101 116\n",
      "102 150\n",
      "103 195\n",
      "104 125\n",
      "105 102\n",
      "106 154\n",
      "107 321\n",
      "108 101\n",
      "109 225\n",
      "110 111\n",
      "111 146\n",
      "112 208\n",
      "113 79\n",
      "114 82\n",
      "115 149\n",
      "116 181\n",
      "117 145\n",
      "118 182\n",
      "119 140\n",
      "120 111\n",
      "121 128\n",
      "122 111\n",
      "123 159\n",
      "124 127\n",
      "125 156\n",
      "126 165\n",
      "127 155\n",
      "128 204\n",
      "129 200\n",
      "130 136\n",
      "131 145\n",
      "132 132\n",
      "133 126\n",
      "134 33\n",
      "135 236\n",
      "136 140\n",
      "137 117\n",
      "138 106\n",
      "139 7\n",
      "140 113\n",
      "141 4\n",
      "142 206\n",
      "143 154\n",
      "144 194\n",
      "145 197\n",
      "146 74\n",
      "147 172\n",
      "148 132\n",
      "149 173\n",
      "150 147\n",
      "151 198\n",
      "152 140\n",
      "153 87\n",
      "154 144\n",
      "155 160\n",
      "156 129\n",
      "157 241\n",
      "158 217\n",
      "159 239\n",
      "160 155\n",
      "161 153\n",
      "162 193\n",
      "163 97\n",
      "164 158\n",
      "165 84\n",
      "166 139\n",
      "167 120\n",
      "168 107\n",
      "169 235\n",
      "170 357\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6117552 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:49\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    995\u001b[0m )\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:402\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    394\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:324\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    321\u001b[0m         relative_position_scores_key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhrd,lrd->bhlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[0;32m    322\u001b[0m         attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m relative_position_scores_query \u001b[38;5;241m+\u001b[39m relative_position_scores_key\n\u001b[1;32m--> 324\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_head_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6117552 bytes."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_embd = {}\n",
    "for ix in df.index:\n",
    "  preprocess = eval(df.loc[ix][\"preprocess\"])\n",
    "  doc_clean = preprocess[\"doc_clean\"]\n",
    "  matching = preprocess[\"matching\"]\n",
    "  uni_stop = max([i for i, v in enumerate(matching) if type(v[0]) == int])+1\n",
    "  doc = \" \".join(doc_clean[:uni_stop])\n",
    "  label = df.loc[ix][\"label\"]\n",
    "\n",
    "  tokens = bert_tokenizer.tokenize(doc)\n",
    "  if len(tokens) > 512:\n",
    "    tokens1 = tokens[:512]\n",
    "    tokens2 = tokens[512:512*2]\n",
    "\n",
    "    tokens_ids1 = bert_tokenizer.convert_tokens_to_ids(tokens1)\n",
    "    tokens_ids1_tensor = torch.tensor(tokens_ids1)\n",
    "    attn_mask1 = (tokens_ids1_tensor != 1).long() # [PAD] => 1\n",
    "\n",
    "    print(ix, len(tokens_ids1))\n",
    "\n",
    "    cont1 = bert_model(tokens_ids1_tensor.unsqueeze(0), attention_mask=attn_mask1.unsqueeze(0))\n",
    "\n",
    "    token_embd_per_doc = []\n",
    "    for i, token in enumerate(tokens1):\n",
    "      embd = cont1.last_hidden_state[0][i].detach().numpy()\n",
    "      token_embd_per_doc.append(embd)\n",
    "\n",
    "    tokens_ids2 = bert_tokenizer.convert_tokens_to_ids(tokens2)\n",
    "    tokens_ids2_tensor = torch.tensor(tokens_ids2)\n",
    "    attn_mask2 = (tokens_ids2_tensor != 1).long() # [PAD] => 1\n",
    "\n",
    "    print(ix, len(tokens_ids2))\n",
    "\n",
    "    cont2 = bert_model(tokens_ids2_tensor.unsqueeze(0), attention_mask=attn_mask2.unsqueeze(0))\n",
    "\n",
    "    for i, token in enumerate(tokens2):\n",
    "      embd = cont2.last_hidden_state[0][i].detach().numpy()\n",
    "      token_embd_per_doc.append(embd)\n",
    "\n",
    "    token_embd[label] = (tokens, token_embd_per_doc)\n",
    "    \n",
    "  else:\n",
    "    tokens_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "    attn_mask = (tokens_ids_tensor != 1).long() # [PAD] => 1\n",
    "\n",
    "    print(ix, len(tokens_ids))\n",
    "\n",
    "    cont = bert_model(tokens_ids_tensor.unsqueeze(0), attention_mask=attn_mask.unsqueeze(0))\n",
    "\n",
    "\n",
    "    token_embd_per_doc = []\n",
    "    for i, token in enumerate(tokens):\n",
    "      embd = cont.last_hidden_state[0][i].detach().numpy()\n",
    "      token_embd_per_doc.append(embd)\n",
    "\n",
    "    token_embd[label] = (tokens, token_embd_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fbba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token_embd, open(\"token_embd.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa5a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = {}\n",
    "count_term = {}\n",
    "for label, (tokens, token_embd_per_doc) in token_embd.items():\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in term_matrix.keys():\n",
    "            term_matrix[token] = np.zeros(768)\n",
    "            count_term[token] = 0\n",
    "        term_matrix[token] += token_embd_per_doc[i]\n",
    "        count_term[token] += 1\n",
    "term_matrix = {k: v / count_term[token] for k, v in term_matrix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "018f5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = pd.DataFrame(term_matrix).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9eb79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6280671",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans =  KMeans(n_clusters=6, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e5271d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=6, random_state=2022)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f70ba02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Students</th>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>-0.001253</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>-0.001387</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>-0.000931</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.009399</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>0.008293</td>\n",
       "      <td>-0.008916</td>\n",
       "      <td>0.008366</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>0.034617</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.007236</td>\n",
       "      <td>-0.004155</td>\n",
       "      <td>-0.014623</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>-0.003325</td>\n",
       "      <td>-0.014523</td>\n",
       "      <td>-0.001164</td>\n",
       "      <td>-0.013144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.064246</td>\n",
       "      <td>-0.051398</td>\n",
       "      <td>0.080492</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>0.039552</td>\n",
       "      <td>-0.013518</td>\n",
       "      <td>-0.078758</td>\n",
       "      <td>0.025755</td>\n",
       "      <td>0.059910</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>-0.010559</td>\n",
       "      <td>0.049799</td>\n",
       "      <td>0.022323</td>\n",
       "      <td>-0.046592</td>\n",
       "      <td>-0.055785</td>\n",
       "      <td>-0.013861</td>\n",
       "      <td>-0.046800</td>\n",
       "      <td>0.051374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sustainability</th>\n",
       "      <td>0.074363</td>\n",
       "      <td>-0.018554</td>\n",
       "      <td>-0.047601</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>-0.040669</td>\n",
       "      <td>-0.041887</td>\n",
       "      <td>-0.031234</td>\n",
       "      <td>0.043220</td>\n",
       "      <td>0.048993</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>-0.002584</td>\n",
       "      <td>-0.115838</td>\n",
       "      <td>-0.042232</td>\n",
       "      <td>-0.015646</td>\n",
       "      <td>-0.035019</td>\n",
       "      <td>-0.006102</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>0.032001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–</th>\n",
       "      <td>0.005444</td>\n",
       "      <td>-0.002325</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>-0.001276</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.003996</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>-0.005121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>-0.003512</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>-0.000795</td>\n",
       "      <td>-0.002539</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>-0.000239</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>-0.000212</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vice</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versa</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000243</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interesting</th>\n",
       "      <td>0.000152</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.000239</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##wer</th>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.000502</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4975 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1         2         3         4         5    \\\n",
       "Students       -0.000023 -0.001884  0.001984 -0.000670 -0.000614  0.000729   \n",
       "’               0.009399 -0.001880  0.008293 -0.008916  0.008366  0.004086   \n",
       "learning        0.064246 -0.051398  0.080492  0.047074  0.039552 -0.013518   \n",
       "sustainability  0.074363 -0.018554 -0.047601  0.003793 -0.040669 -0.041887   \n",
       "–               0.005444 -0.002325  0.000231 -0.001276  0.004283 -0.003996   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "thing           0.000243  0.000036  0.000284 -0.000239  0.000264  0.000037   \n",
       "vice            0.000147 -0.000007 -0.000057 -0.000512  0.000140 -0.000134   \n",
       "versa          -0.000021 -0.000019  0.000171 -0.000103  0.000201  0.000221   \n",
       "interesting     0.000152 -0.000208  0.000217  0.000134  0.000104 -0.000239   \n",
       "##wer          -0.000071 -0.000140 -0.000142 -0.000328  0.000580 -0.000137   \n",
       "\n",
       "                     6         7         8         9    ...       758  \\\n",
       "Students       -0.001253  0.000591 -0.001387  0.000122  ...  0.000723   \n",
       "’               0.000874  0.008740  0.034617  0.009243  ...  0.008519   \n",
       "learning       -0.078758  0.025755  0.059910  0.012195  ...  0.006202   \n",
       "sustainability -0.031234  0.043220  0.048993  0.001243  ... -0.037491   \n",
       "–               0.000014 -0.000946  0.006156 -0.005121  ...  0.003760   \n",
       "...                  ...       ...       ...       ...  ...       ...   \n",
       "thing          -0.000303  0.000439  0.000094  0.000035  ...  0.000314   \n",
       "vice           -0.000129  0.000177  0.000209 -0.000396  ... -0.000287   \n",
       "versa           0.000012  0.000184 -0.000058 -0.000004  ... -0.000162   \n",
       "interesting    -0.000125 -0.000135  0.000389  0.000127  ...  0.000213   \n",
       "##wer          -0.000172  0.000105 -0.000053  0.000187  ...  0.000032   \n",
       "\n",
       "                     759       760       761       762       763       764  \\\n",
       "Students       -0.000224  0.002051 -0.000931  0.000453  0.001093 -0.001995   \n",
       "’              -0.000262 -0.007236 -0.004155 -0.014623  0.022284 -0.003325   \n",
       "learning        0.015610 -0.010559  0.049799  0.022323 -0.046592 -0.055785   \n",
       "sustainability -0.002584 -0.115838 -0.042232 -0.015646 -0.035019 -0.006102   \n",
       "–              -0.000655 -0.003512 -0.001402  0.001377  0.003986 -0.000795   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "thing          -0.000294  0.000190 -0.000053  0.000302 -0.000212 -0.000229   \n",
       "vice            0.000337 -0.000104 -0.000776 -0.000137 -0.000040 -0.000021   \n",
       "versa          -0.000243 -0.000200 -0.000110  0.000163  0.000269  0.000116   \n",
       "interesting     0.000067 -0.000081 -0.000043 -0.000047 -0.000170  0.000106   \n",
       "##wer          -0.000141  0.000139 -0.000502  0.000178  0.000247  0.000217   \n",
       "\n",
       "                     765       766       767  \n",
       "Students       -0.001522  0.000096  0.000250  \n",
       "’              -0.014523 -0.001164 -0.013144  \n",
       "learning       -0.013861 -0.046800  0.051374  \n",
       "sustainability  0.028133  0.011089  0.032001  \n",
       "–              -0.002539  0.001337  0.001926  \n",
       "...                  ...       ...       ...  \n",
       "thing           0.000044  0.000103  0.000010  \n",
       "vice           -0.000073 -0.000086 -0.000334  \n",
       "versa           0.000157  0.000339 -0.000003  \n",
       "interesting    -0.000004 -0.000033  0.000342  \n",
       "##wer           0.000414 -0.000304 -0.000005  \n",
       "\n",
       "[4975 rows x 768 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b4d56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.predict(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd13d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for i, l in enumerate(labels):\n",
    "    term = terms.index[i]\n",
    "    if l not in clusters.keys():\n",
    "        clusters[l] = []\n",
    "    clusters[l].append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1475291f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['Students',\n",
       "  '–',\n",
       "  'imp',\n",
       "  '##licit',\n",
       "  'explicit',\n",
       "  'or',\n",
       "  'non',\n",
       "  'exist',\n",
       "  '##ent',\n",
       "  'case',\n",
       "  'addressing',\n",
       "  'SD',\n",
       "  '##G',\n",
       "  'H',\n",
       "  '##EI',\n",
       "  'program',\n",
       "  'P',\n",
       "  '##ur',\n",
       "  '##pose',\n",
       "  'aims',\n",
       "  'understand',\n",
       "  'better',\n",
       "  'student',\n",
       "  'awareness',\n",
       "  'knowledge',\n",
       "  'how',\n",
       "  'Sustainable',\n",
       "  'Development',\n",
       "  'Goals',\n",
       "  'used',\n",
       "  'higher',\n",
       "  'institutions',\n",
       "  'm',\n",
       "  '##ot',\n",
       "  '##ivate',\n",
       "  'It',\n",
       "  'essential',\n",
       "  'consider',\n",
       "  'understanding',\n",
       "  'at',\n",
       "  'end',\n",
       "  'studies',\n",
       "  'assess',\n",
       "  'whether',\n",
       "  'they',\n",
       "  'feel',\n",
       "  'prepared',\n",
       "  'apply',\n",
       "  'daily',\n",
       "  'work',\n",
       "  'life',\n",
       "  'Design',\n",
       "  '/',\n",
       "  'methodology',\n",
       "  'has',\n",
       "  'quantitative',\n",
       "  'design',\n",
       "  'specific',\n",
       "  'method',\n",
       "  'online',\n",
       "  'survey',\n",
       "  'masters',\n",
       "  'using',\n",
       "  'university',\n",
       "  'platform',\n",
       "  'Eva',\n",
       "  '##S',\n",
       "  '##ys',\n",
       "  '##es',\n",
       "  'approaching',\n",
       "  'perceive',\n",
       "  'overall',\n",
       "  'integrating',\n",
       "  'into',\n",
       "  'programs',\n",
       "  'cu',\n",
       "  '##rri',\n",
       "  '##cula',\n",
       "  'Finding',\n",
       "  'results',\n",
       "  'showed',\n",
       "  'integrated',\n",
       "  'W',\n",
       "  '##IL',\n",
       "  'projects',\n",
       "  'real',\n",
       "  'experiences',\n",
       "  'part',\n",
       "  'enhanced',\n",
       "  'Moreover',\n",
       "  'offers',\n",
       "  'universities',\n",
       "  'way',\n",
       "  'frame',\n",
       "  'ways',\n",
       "  'allow',\n",
       "  'them',\n",
       "  'develop',\n",
       "  'inter',\n",
       "  '##person',\n",
       "  '##al',\n",
       "  'ambassador',\n",
       "  'future',\n",
       "  '##ractical',\n",
       "  'implications',\n",
       "  'supports',\n",
       "  'argument',\n",
       "  'enhance',\n",
       "  'critical',\n",
       "  'Original',\n",
       "  '##ity',\n",
       "  'value',\n",
       "  'p',\n",
       "  '##eda',\n",
       "  '##go',\n",
       "  '##gical',\n",
       "  'advanced',\n",
       "  'paper',\n",
       "  'addresses',\n",
       "  'might',\n",
       "  'indicates',\n",
       "  'working',\n",
       "  'encourages',\n",
       "  'promote',\n",
       "  '202',\n",
       "  '##1',\n",
       "  'Karin',\n",
       "  'Al',\n",
       "  '##m',\n",
       "  'Thomas',\n",
       "  'Beer',\n",
       "  '##y',\n",
       "  'David',\n",
       "  'E',\n",
       "  '##ib',\n",
       "  '##lm',\n",
       "  '##ei',\n",
       "  '##er',\n",
       "  'Ta',\n",
       "  '##rek',\n",
       "  'F',\n",
       "  '##ah',\n",
       "  '##my',\n",
       "  'Higher',\n",
       "  'University',\n",
       "  'social',\n",
       "  'responsibility',\n",
       "  'US',\n",
       "  '##R',\n",
       "  'Work',\n",
       "  'Learning',\n",
       "  '##ete',\n",
       "  '##nce',\n",
       "  'approaches',\n",
       "  'organizational',\n",
       "  'individual',\n",
       "  'context',\n",
       "  'With',\n",
       "  'increasing',\n",
       "  'pace',\n",
       "  'digital',\n",
       "  '##ization',\n",
       "  'auto',\n",
       "  '##mat',\n",
       "  'robot',\n",
       "  'firms',\n",
       "  'need',\n",
       "  'quickly',\n",
       "  'anti',\n",
       "  '##ci',\n",
       "  '##pate',\n",
       "  'new',\n",
       "  'consumer',\n",
       "  'values',\n",
       "  'trends',\n",
       "  'needs',\n",
       "  'adjust',\n",
       "  'production',\n",
       "  'thus',\n",
       "  'requiring',\n",
       "  'constant',\n",
       "  'adaptation',\n",
       "  'models',\n",
       "  'review',\n",
       "  '1',\n",
       "  'briefly',\n",
       "  'outlines',\n",
       "  '2',\n",
       "  'presents',\n",
       "  'difficulties',\n",
       "  'when',\n",
       "  'defining',\n",
       "  'del',\n",
       "  '##ine',\n",
       "  '##ating',\n",
       "  'differences',\n",
       "  'between',\n",
       "  'skills',\n",
       "  'abilities',\n",
       "  '3',\n",
       "  'main',\n",
       "  'challenges',\n",
       "  'measurement',\n",
       "  '4',\n",
       "  'recent',\n",
       "  'findings',\n",
       "  '5',\n",
       "  'highlights',\n",
       "  'these',\n",
       "  'likely',\n",
       "  'encounter',\n",
       "  'To',\n",
       "  'systematic',\n",
       "  'literature',\n",
       "  'was',\n",
       "  'carried',\n",
       "  'out',\n",
       "  'sum',\n",
       "  '##mar',\n",
       "  '##ize',\n",
       "  'relevant',\n",
       "  'published',\n",
       "  'define',\n",
       "  'directions',\n",
       "  'Results',\n",
       "  'show',\n",
       "  'further',\n",
       "  'should',\n",
       "  'focus',\n",
       "  'general',\n",
       "  '##izing',\n",
       "  'looking',\n",
       "  'various',\n",
       "  'groups',\n",
       "  'workers',\n",
       "  'industries',\n",
       "  'expanding',\n",
       "  'set',\n",
       "  'analyses',\n",
       "  'different',\n",
       "  'definitions',\n",
       "  'developing',\n",
       "  'alternative',\n",
       "  'impact',\n",
       "  'performance',\n",
       "  'Such',\n",
       "  'would',\n",
       "  'employees',\n",
       "  '##ness',\n",
       "  'terms',\n",
       "  'requirements',\n",
       "  'well',\n",
       "  'identify',\n",
       "  'most',\n",
       "  'gaps',\n",
       "  'opportunities',\n",
       "  'formal',\n",
       "  'educational',\n",
       "  'system',\n",
       "  'job',\n",
       "  '##2',\n",
       "  'Author',\n",
       "  's',\n",
       "  'Concept',\n",
       "  '##ual',\n",
       "  'construction',\n",
       "  'global',\n",
       "  'Global',\n",
       "  'objective',\n",
       "  'become',\n",
       "  'since',\n",
       "  'its',\n",
       "  'inclusion',\n",
       "  'Programme',\n",
       "  'International',\n",
       "  'Student',\n",
       "  'Assessment',\n",
       "  '##IS',\n",
       "  '##A',\n",
       "  'Despite',\n",
       "  'growing',\n",
       "  'interest',\n",
       "  'there',\n",
       "  'several',\n",
       "  'issues',\n",
       "  'deep',\n",
       "  'reflection',\n",
       "  'such',\n",
       "  'What',\n",
       "  '?',\n",
       "  'How',\n",
       "  'it',\n",
       "  'been',\n",
       "  'constructed',\n",
       "  'From',\n",
       "  'disc',\n",
       "  '##urs',\n",
       "  '##ive',\n",
       "  'processes',\n",
       "  'And',\n",
       "  'what',\n",
       "  'does',\n",
       "  '##cultural',\n",
       "  'contribute',\n",
       "  'construct',\n",
       "  'answer',\n",
       "  'questions',\n",
       "  'background',\n",
       "  'conceptual',\n",
       "  '##isation',\n",
       "  'theories',\n",
       "  'concept',\n",
       "  'built',\n",
       "  'special',\n",
       "  'emphasis',\n",
       "  'model',\n",
       "  'favour',\n",
       "  'analysis',\n",
       "  'found',\n",
       "  'instrumental',\n",
       "  '##ist',\n",
       "  'efficiency',\n",
       "  'pre',\n",
       "  '##dom',\n",
       "  '##inated',\n",
       "  'least',\n",
       "  'underlying',\n",
       "  'opposed',\n",
       "  're',\n",
       "  '##ism',\n",
       "  'usually',\n",
       "  'appears',\n",
       "  'fore',\n",
       "  '##ground',\n",
       "  'On',\n",
       "  'many',\n",
       "  'occasions',\n",
       "  'interchange',\n",
       "  '##ably',\n",
       "  'although',\n",
       "  'former',\n",
       "  'more',\n",
       "  'comprehensive',\n",
       "  'Finally',\n",
       "  'highlighted',\n",
       "  'limitations',\n",
       "  'make',\n",
       "  'difficult',\n",
       "  'compare',\n",
       "  'acquisition',\n",
       "  'international',\n",
       "  'level',\n",
       "  'therefore',\n",
       "  'require',\n",
       "  'systematically',\n",
       "  'investigate',\n",
       "  'Ed',\n",
       "  '##ici',\n",
       "  '##ones',\n",
       "  'Universidad',\n",
       "  'de',\n",
       "  'Sal',\n",
       "  '##aman',\n",
       "  '##ca',\n",
       "  'All',\n",
       "  'rights',\n",
       "  'reserved',\n",
       "  'Educational',\n",
       "  'citizenship',\n",
       "  'Inter',\n",
       "  'Trans',\n",
       "  '##form',\n",
       "  '##ative',\n",
       "  'Dev',\n",
       "  '##elo',\n",
       "  '##ping',\n",
       "  'through',\n",
       "  'project',\n",
       "  'graduate',\n",
       "  'role',\n",
       "  'within',\n",
       "  'lens',\n",
       "  'Project',\n",
       "  'widely',\n",
       "  'recommended',\n",
       "  '##gy',\n",
       "  'h',\n",
       "  '##pot',\n",
       "  '##hes',\n",
       "  '##ized',\n",
       "  'collaboration',\n",
       "  'autonomy',\n",
       "  'world',\n",
       "  'application',\n",
       "  'also',\n",
       "  'examine',\n",
       "  'connection',\n",
       "  'perspective',\n",
       "  'two',\n",
       "  'year',\n",
       "  'comparative',\n",
       "  'follows',\n",
       "  'journeys',\n",
       "  'nine',\n",
       "  'three',\n",
       "  'Master',\n",
       "  'Su',\n",
       "  '##sta',\n",
       "  '##ina',\n",
       "  '##bility',\n",
       "  'Arizona',\n",
       "  'State',\n",
       "  'Science',\n",
       "  'Le',\n",
       "  '##up',\n",
       "  '##hana',\n",
       "  'L',\n",
       "  '##ü',\n",
       "  '##ne',\n",
       "  '##burg',\n",
       "  'AS',\n",
       "  '##U',\n",
       "  'Over',\n",
       "  'four',\n",
       "  'semester',\n",
       "  'each',\n",
       "  'took',\n",
       "  'oriented',\n",
       "  'self',\n",
       "  'assessments',\n",
       "  'interviews',\n",
       "  'map',\n",
       "  'perceived',\n",
       "  'throughout',\n",
       "  'Additional',\n",
       "  'information',\n",
       "  'gathered',\n",
       "  'course',\n",
       "  'materials',\n",
       "  'descriptions',\n",
       "  'instructor',\n",
       "  'v',\n",
       "  '##ivo',\n",
       "  'observations',\n",
       "  'aspects',\n",
       "  'including',\n",
       "  'do',\n",
       "  'driven',\n",
       "  'equally',\n",
       "  'foster',\n",
       "  'result',\n",
       "  'long',\n",
       "  'balancing',\n",
       "  'support',\n",
       "  'independence',\n",
       "  'associated',\n",
       "  'mit',\n",
       "  '##igate',\n",
       "  '##d',\n",
       "  'actions',\n",
       "  'cop',\n",
       "  '##ing',\n",
       "  'can',\n",
       "  'designing',\n",
       "  'aimed',\n",
       "  '##rricular',\n",
       "  'combined',\n",
       "  'repeated',\n",
       "  'overtime',\n",
       "  'reported',\n",
       "  '##tribution',\n",
       "  'courses',\n",
       "  'activities',\n",
       "  'bridges',\n",
       "  'gap',\n",
       "  'theoretical',\n",
       "  'recommendations',\n",
       "  'In',\n",
       "  'addition',\n",
       "  'length',\n",
       "  'depth',\n",
       "  'forefront',\n",
       "  'experience',\n",
       "  'delivered',\n",
       "  'Emerald',\n",
       "  'Publishing',\n",
       "  'Limited',\n",
       "  'C',\n",
       "  '##ric',\n",
       "  '##ulum',\n",
       "  'Education',\n",
       "  'Key',\n",
       "  'Professional',\n",
       "  'An',\n",
       "  'Em',\n",
       "  '##pi',\n",
       "  '##rical',\n",
       "  'Study',\n",
       "  'empirical',\n",
       "  'period',\n",
       "  'years',\n",
       "  'we',\n",
       "  'collected',\n",
       "  'data',\n",
       "  'senior',\n",
       "  'professionals',\n",
       "  'action',\n",
       "  'Data',\n",
       "  'collection',\n",
       "  'place',\n",
       "  'Belgium',\n",
       "  'Flanders',\n",
       "  'via',\n",
       "  'elaborate',\n",
       "  '##erative',\n",
       "  'process',\n",
       "  'interactive',\n",
       "  'workshop',\n",
       "  'Our',\n",
       "  'provides',\n",
       "  'strong',\n",
       "  'evidence',\n",
       "  'existence',\n",
       "  'com',\n",
       "  '##ple',\n",
       "  '##men',\n",
       "  'ta',\n",
       "  '##ry',\n",
       "  '##ncy',\n",
       "  'clusters',\n",
       "  'namely',\n",
       "  'intervention',\n",
       "  'Together',\n",
       "  'enable',\n",
       "  'profound',\n",
       "  'ensure',\n",
       "  'ability',\n",
       "  '##vise',\n",
       "  'solutions',\n",
       "  'change',\n",
       "  'towards',\n",
       "  'Furthermore',\n",
       "  'shed',\n",
       "  'first',\n",
       "  'light',\n",
       "  'interaction',\n",
       "  'link',\n",
       "  'practitioners',\n",
       "  'topic',\n",
       "  'lived',\n",
       "  'diver',\n",
       "  '##gence',\n",
       "  '##ly',\n",
       "  'novel',\n",
       "  'insights',\n",
       "  'fields',\n",
       "  'science',\n",
       "  'human',\n",
       "  'resource',\n",
       "  'management',\n",
       "  'authors',\n",
       "  'License',\n",
       "  '##e',\n",
       "  'MD',\n",
       "  '##PI',\n",
       "  'Basel',\n",
       "  'Switzerland',\n",
       "  'transformation',\n",
       "  'Coach',\n",
       "  'prepare',\n",
       "  'school',\n",
       "  'transition',\n",
       "  'core',\n",
       "  'coaching',\n",
       "  'present',\n",
       "  'proposes',\n",
       "  'labour',\n",
       "  'market',\n",
       "  'Taking',\n",
       "  'proposed',\n",
       "  'practice',\n",
       "  'employ',\n",
       "  '##ability',\n",
       "  'facilitate',\n",
       "  'smooth',\n",
       "  'However',\n",
       "  'looks',\n",
       "  'like',\n",
       "  'remains',\n",
       "  'largely',\n",
       "  'unclear',\n",
       "  'expected',\n",
       "  'highly',\n",
       "  'skilled',\n",
       "  'coaches',\n",
       "  'f',\n",
       "  '##ac',\n",
       "  '##ilitating',\n",
       "  'coach',\n",
       "  'A',\n",
       "  'q',\n",
       "  '##itative',\n",
       "  'adopted',\n",
       "  'were',\n",
       "  'consisting',\n",
       "  'workplace',\n",
       "  'create',\n",
       "  'necessary',\n",
       "  'conditions',\n",
       "  'creates',\n",
       "  'safe',\n",
       "  'environment',\n",
       "  'setting',\n",
       "  'goals',\n",
       "  'guide',\n",
       "  'undertake',\n",
       "  'attain',\n",
       "  'asks',\n",
       "  'reflective',\n",
       "  'stimulate',\n",
       "  'ownership',\n",
       "  'putting',\n",
       "  'centre',\n",
       "  'decision',\n",
       "  'making',\n",
       "  'emphasize',\n",
       "  'importance',\n",
       "  'professional',\n",
       "  'attitude',\n",
       "  'about',\n",
       "  'article',\n",
       "  'concludes',\n",
       "  'practical',\n",
       "  'no',\n",
       "  '##vice',\n",
       "  'adds',\n",
       "  'agenda',\n",
       "  'readiness',\n",
       "  'proposing',\n",
       "  'preparing',\n",
       "  'Ni',\n",
       "  '##els',\n",
       "  'van',\n",
       "  'der',\n",
       "  'Ba',\n",
       "  '##an',\n",
       "  '##ken',\n",
       "  'Gas',\n",
       "  '##t',\n",
       "  '##im',\n",
       "  'G',\n",
       "  '##i',\n",
       "  '##j',\n",
       "  '##sel',\n",
       "  '##ae',\n",
       "  '##rs',\n",
       "  'Simon',\n",
       "  'Beau',\n",
       "  '##sa',\n",
       "  '##ert',\n",
       "  '##p',\n",
       "  '##loy',\n",
       "  'School',\n",
       "  '##place',\n",
       "  'Re',\n",
       "  '##co',\n",
       "  '##gni',\n",
       "  '##sing',\n",
       "  'measuring',\n",
       "  'natural',\n",
       "  'hazard',\n",
       "  'preparation',\n",
       "  'index',\n",
       "  'weather',\n",
       "  'related',\n",
       "  'hazards',\n",
       "  'number',\n",
       "  'severity',\n",
       "  'important',\n",
       "  'than',\n",
       "  'ever',\n",
       "  'communities',\n",
       "  'all',\n",
       "  'types',\n",
       "  'not',\n",
       "  'reveal',\n",
       "  'much',\n",
       "  'enough',\n",
       "  'con',\n",
       "  '##verse',\n",
       "  'low',\n",
       "  'levels',\n",
       "  'easily',\n",
       "  'recognised',\n",
       "  'emergency',\n",
       "  'agencies',\n",
       "  'maps',\n",
       "  'Australian',\n",
       "  'agency',\n",
       "  'needed',\n",
       "  'individuals',\n",
       "  'effective',\n",
       "  'Using',\n",
       "  'semi',\n",
       "  'structured',\n",
       "  '30',\n",
       "  'local',\n",
       "  'council',\n",
       "  'profit',\n",
       "  'organisation',\n",
       "  'staff',\n",
       "  'states',\n",
       "  'participants',\n",
       "  'identified',\n",
       "  'range',\n",
       "  'community',\n",
       "  'features',\n",
       "  'had',\n",
       "  'seen',\n",
       "  'un',\n",
       "  'believed',\n",
       "  'protective',\n",
       "  'These',\n",
       "  'then',\n",
       "  'mapped',\n",
       "  'against',\n",
       "  'perception',\n",
       "  'five',\n",
       "  'resulting',\n",
       "  'Pre',\n",
       "  '##par',\n",
       "  '##ed',\n",
       "  'Index',\n",
       "  'allows',\n",
       "  'bench',\n",
       "  '##mark',\n",
       "  'recognise',\n",
       "  'lack',\n",
       "  'leaves',\n",
       "  'vulnerable',\n",
       "  'Community',\n",
       "  'Natural',\n",
       "  'Pro',\n",
       "  '##tec',\n",
       "  '##tive',\n",
       "  'S',\n",
       "  '##hare',\n",
       "  'Ai',\n",
       "  '##ms',\n",
       "  'Trust',\n",
       "  '##ass',\n",
       "  '##ion',\n",
       "  '##ow',\n",
       "  'People',\n",
       "  '##sper',\n",
       "  'Teacher',\n",
       "  '##uca',\n",
       "  '##tors',\n",
       "  '##´',\n",
       "  'Life',\n",
       "  '##long',\n",
       "  'Teachers',\n",
       "  'occupational',\n",
       "  'being',\n",
       "  'significant',\n",
       "  'promoting',\n",
       "  'aim',\n",
       "  'determine',\n",
       "  'factors',\n",
       "  'educators',\n",
       "  'commitment',\n",
       "  'give',\n",
       "  'energy',\n",
       "  'consisted',\n",
       "  '24',\n",
       "  'O',\n",
       "  '##ulu',\n",
       "  'Applied',\n",
       "  'Sciences',\n",
       "  '##atic',\n",
       "  'content',\n",
       "  '##hen',\n",
       "  '##ome',\n",
       "  '##no',\n",
       "  '##graphy',\n",
       "  'single',\n",
       "  'factor',\n",
       "  'seemed',\n",
       "  'administrative',\n",
       "  'included',\n",
       "  'dialogue',\n",
       "  '##otion',\n",
       "  'meaningful',\n",
       "  'play',\n",
       "  'often',\n",
       "  'fellowship',\n",
       "  '##mm',\n",
       "  '##itte',\n",
       "  'take',\n",
       "  'culture',\n",
       "  'Po',\n",
       "  '##sitive',\n",
       "  'attitudes',\n",
       "  'motivation',\n",
       "  'seem',\n",
       "  'connected',\n",
       "  'capability',\n",
       "  'et',\n",
       "  '##hos',\n",
       "  'passion',\n",
       "  'Dialogue',\n",
       "  'identity',\n",
       "  'So',\n",
       "  '##cio',\n",
       "  '##formation',\n",
       "  'Per',\n",
       "  '##spective',\n",
       "  '—',\n",
       "  'I',\n",
       "  '##mple',\n",
       "  '##mentation',\n",
       "  '##ula',\n",
       "  'field',\n",
       "  'environmental',\n",
       "  'protection',\n",
       "  'activity',\n",
       "  'operation',\n",
       "  'satisfaction',\n",
       "  'generations',\n",
       "  'face',\n",
       "  'changes',\n",
       "  'supporting',\n",
       "  'innovative',\n",
       "  'economy',\n",
       "  'young',\n",
       "  'people',\n",
       "  'opinion',\n",
       "  'implementation',\n",
       "  'publication',\n",
       "  'responds',\n",
       "  'demand',\n",
       "  'technical',\n",
       "  'greater',\n",
       "  'integration',\n",
       "  'economic',\n",
       "  'En',\n",
       "  '##ct',\n",
       "  'AG',\n",
       "  '##H',\n",
       "  'Technology',\n",
       "  'K',\n",
       "  '##rak',\n",
       "  'Poland',\n",
       "  'shows',\n",
       "  'increasingly',\n",
       "  'aware',\n",
       "  'lives',\n",
       "  'both',\n",
       "  'raising',\n",
       "  'society',\n",
       "  'conducted',\n",
       "  'allowed',\n",
       "  'us',\n",
       "  'did',\n",
       "  '##actic',\n",
       "  'graduates',\n",
       "  'raw',\n",
       "  'faculties',\n",
       "  'assessment',\n",
       "  'Raw',\n",
       "  'public',\n",
       "  '##rac',\n",
       "  'Mal',\n",
       "  '##op',\n",
       "  '##ols',\n",
       "  '##kie',\n",
       "  '[',\n",
       "  'Central',\n",
       "  'Europe',\n",
       "  ']',\n",
       "  '##nces',\n",
       "  'Fu',\n",
       "  '##zzy',\n",
       "  'Model',\n",
       "  '##pp',\n",
       "  '##roach',\n",
       "  'Contemporary',\n",
       "  'goal',\n",
       "  'provide',\n",
       "  'determined',\n",
       "  'legislative',\n",
       "  'demands',\n",
       "  'graduated',\n",
       "  'intending',\n",
       "  'strengthen',\n",
       "  'idea',\n",
       "  'lifelong',\n",
       "  'LL',\n",
       "  '##L',\n",
       "  'Since',\n",
       "  'very',\n",
       "  'complex',\n",
       "  'task',\n",
       "  'characterized',\n",
       "  'vague',\n",
       "  'inherent',\n",
       "  'subjective',\n",
       "  'nature',\n",
       "  'thinking',\n",
       "  'uncertain',\n",
       "  'assessed',\n",
       "  'represented',\n",
       "  'linguistic',\n",
       "  'expressions',\n",
       "  'modeled',\n",
       "  'interval',\n",
       "  'type',\n",
       "  'trap',\n",
       "  '##ez',\n",
       "  '##oid',\n",
       "  'fuzzy',\n",
       "  'numbers',\n",
       "  'Ranking',\n",
       "  'stated',\n",
       "  'multi',\n",
       "  'criteria',\n",
       "  'optimization',\n",
       "  '##gg',\n",
       "  '##regation',\n",
       "  'opinions',\n",
       "  'unique',\n",
       "  'marks',\n",
       "  'given',\n",
       "  'Del',\n",
       "  '##phi',\n",
       "  'technique',\n",
       "  'rank',\n",
       "  'Simple',\n",
       "  'Ad',\n",
       "  '##dit',\n",
       "  'Weight',\n",
       "  '##SA',\n",
       "  '##W',\n",
       "  'order',\n",
       "  'measures',\n",
       "  'improve',\n",
       "  'obtained',\n",
       "  'tested',\n",
       "  'discussed',\n",
       "  'sample',\n",
       "  'proposal',\n",
       "  'engineers',\n",
       "  'SA',\n",
       "  'accuracy',\n",
       "  '##c',\n",
       "  '##rite',\n",
       "  '##ria',\n",
       "  'Foster',\n",
       "  'Mo',\n",
       "  'Knowledge',\n",
       "  'Skills',\n",
       "  'At',\n",
       "  '##ti',\n",
       "  '##tudes',\n",
       "  'mobility',\n",
       "  'programmes',\n",
       "  'em',\n",
       "  '##bed',\n",
       "  '##E',\n",
       "  'gain',\n",
       "  'some',\n",
       "  'crucial',\n",
       "  'pursuing',\n",
       "  'We',\n",
       "  'propose',\n",
       "  'framework',\n",
       "  'links',\n",
       "  'UNESCO',\n",
       "  'anal',\n",
       "  'addressed',\n",
       "  'institutional',\n",
       "  'initiatives',\n",
       "  'exposing',\n",
       "  'contribution',\n",
       "  'revised',\n",
       "  'mobile',\n",
       "  'but',\n",
       "  'mainly',\n",
       "  '##4',\n",
       "  'Quality',\n",
       "  '##8',\n",
       "  'Dec',\n",
       "  'Economic',\n",
       "  'Growth',\n",
       "  'Not',\n",
       "  '##with',\n",
       "  '##standing',\n",
       "  'additional',\n",
       "  'may',\n",
       "  'leverage',\n",
       "  ...],\n",
       " 3: ['’',\n",
       "  'learning',\n",
       "  'sustainability',\n",
       "  ':',\n",
       "  'study',\n",
       "  'approach',\n",
       "  'on',\n",
       "  'students',\n",
       "  'key',\n",
       "  'compete',\n",
       "  '##ncies',\n",
       "  'This',\n",
       "  '(',\n",
       "  ')',\n",
       "  'are',\n",
       "  'is',\n",
       "  'their',\n",
       "  'The',\n",
       "  'an',\n",
       "  'with',\n",
       "  'that',\n",
       "  'as',\n",
       "  'teaching',\n",
       "  'this',\n",
       "  '©',\n",
       "  'Co',\n",
       "  '##mp',\n",
       "  'based',\n",
       "  'development',\n",
       "  'research',\n",
       "  'by',\n",
       "  'training',\n",
       "  'which',\n",
       "  'from',\n",
       "  'curriculum',\n",
       "  'sustainable',\n",
       "  \"'\",\n",
       "  'teachers',\n",
       "  'be',\n",
       "  'teacher',\n",
       "  'vocational'],\n",
       " 1: [',', 'the', 'and', 'of'],\n",
       " 4: ['-', 'a', '##s', 'in', 'to', 'education', 'for', 'competence'],\n",
       " 2: ['.'],\n",
       " 5: [';']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
