{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8f4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03886966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ace618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb48b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "e = bert_model.eval()\n",
    "z = bert_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e610c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"preprocess_docs.xlsx\"\n",
    "df = pd.read_excel(file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c2c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 238\n",
      "1 137\n",
      "2 156\n",
      "3 250\n",
      "4 163\n",
      "5 212\n",
      "6 125\n",
      "7 124\n",
      "8 195\n",
      "9 180\n",
      "10 165\n",
      "11 116\n",
      "12 182\n",
      "13 165\n",
      "14 234\n",
      "15 90\n",
      "16 132\n",
      "17 381\n",
      "18 152\n",
      "19 143\n",
      "20 204\n",
      "21 237\n",
      "22 184\n",
      "23 160\n",
      "24 202\n",
      "25 216\n",
      "26 207\n",
      "27 140\n",
      "28 126\n",
      "29 328\n",
      "30 161\n",
      "31 200\n",
      "32 148\n",
      "33 196\n",
      "34 162\n",
      "35 90\n",
      "36 293\n",
      "37 235\n",
      "38 135\n",
      "39 150\n",
      "40 211\n",
      "41 140\n",
      "42 197\n",
      "43 114\n",
      "44 161\n",
      "45 173\n",
      "46 172\n",
      "47 166\n",
      "48 209\n",
      "49 143\n",
      "50 175\n",
      "51 160\n",
      "52 128\n",
      "53 160\n",
      "54 185\n",
      "55 123\n",
      "56 193\n",
      "57 222\n",
      "58 230\n",
      "59 175\n",
      "60 188\n",
      "61 166\n",
      "62 201\n",
      "63 209\n",
      "64 170\n",
      "65 176\n",
      "66 198\n",
      "67 244\n",
      "68 178\n",
      "69 181\n",
      "70 139\n",
      "71 170\n",
      "72 180\n",
      "73 130\n",
      "74 194\n",
      "75 202\n",
      "76 130\n",
      "77 189\n",
      "78 160\n",
      "79 172\n",
      "80 206\n",
      "81 112\n",
      "82 248\n",
      "83 310\n",
      "84 120\n",
      "85 141\n",
      "86 205\n",
      "87 166\n",
      "88 201\n",
      "89 129\n",
      "90 265\n",
      "91 116\n",
      "92 113\n",
      "93 194\n",
      "94 161\n",
      "95 140\n",
      "96 230\n",
      "97 202\n",
      "98 109\n",
      "99 136\n",
      "100 223\n",
      "101 116\n",
      "102 150\n",
      "103 195\n",
      "104 125\n",
      "105 102\n",
      "106 154\n",
      "107 321\n",
      "108 101\n",
      "109 225\n",
      "110 111\n",
      "111 146\n",
      "112 208\n",
      "113 79\n",
      "114 82\n",
      "115 149\n",
      "116 181\n",
      "117 145\n",
      "118 182\n",
      "119 140\n",
      "120 111\n",
      "121 128\n",
      "122 111\n",
      "123 159\n",
      "124 127\n",
      "125 156\n",
      "126 165\n",
      "127 155\n",
      "128 204\n",
      "129 200\n",
      "130 136\n",
      "131 145\n",
      "132 132\n",
      "133 126\n",
      "134 33\n",
      "135 236\n",
      "136 140\n",
      "137 117\n",
      "138 106\n",
      "139 7\n",
      "140 113\n",
      "141 4\n",
      "142 206\n",
      "143 154\n",
      "144 194\n",
      "145 197\n",
      "146 74\n",
      "147 172\n",
      "148 132\n",
      "149 173\n",
      "150 147\n",
      "151 198\n",
      "152 140\n",
      "153 87\n",
      "154 144\n",
      "155 160\n",
      "156 129\n",
      "157 241\n",
      "158 217\n",
      "159 239\n",
      "160 155\n",
      "161 153\n",
      "162 193\n",
      "163 97\n",
      "164 158\n",
      "165 84\n",
      "166 139\n",
      "167 120\n",
      "168 107\n",
      "169 235\n",
      "170 357\n",
      "171 164\n",
      "172 189\n",
      "173 162\n",
      "174 156\n",
      "175 126\n",
      "176 99\n",
      "177 107\n",
      "178 131\n",
      "179 61\n",
      "180 99\n",
      "181 78\n",
      "182 85\n",
      "183 115\n",
      "184 69\n",
      "185 108\n",
      "186 75\n",
      "187 135\n",
      "188 182\n",
      "CPU times: total: 6min 28s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_embd = {}\n",
    "for ix in df.index:\n",
    "  preprocess = eval(df.loc[ix][\"preprocess\"])\n",
    "  doc_clean = preprocess[\"doc_clean\"]\n",
    "  matching = preprocess[\"matching\"]\n",
    "  uni_stop = max([i for i, v in enumerate(matching) if type(v[0]) == int])+1\n",
    "  doc = \" \".join(doc_clean[:uni_stop])\n",
    "  label = df.loc[ix][\"label\"]\n",
    "\n",
    "  tokens = bert_tokenizer.tokenize(doc)\n",
    "  if len(tokens) > 512:\n",
    "    tokens1 = tokens[:512]\n",
    "    tokens2 = tokens[512:512*2]\n",
    "\n",
    "    tokens_ids1 = bert_tokenizer.convert_tokens_to_ids(tokens1)\n",
    "    tokens_ids1_tensor = torch.tensor(tokens_ids1)\n",
    "    attn_mask1 = (tokens_ids1_tensor != 1).long() # [PAD] => 1\n",
    "\n",
    "    print(ix, len(tokens_ids1))\n",
    "\n",
    "    cont1 = bert_model(tokens_ids1_tensor.unsqueeze(0), attention_mask=attn_mask1.unsqueeze(0))\n",
    "\n",
    "    token_embd_per_doc = []\n",
    "    for i, token in enumerate(tokens1):\n",
    "      embd = cont1.last_hidden_state[0][i].detach().numpy()\n",
    "      token_embd_per_doc.append(embd)\n",
    "\n",
    "    tokens_ids2 = bert_tokenizer.convert_tokens_to_ids(tokens2)\n",
    "    tokens_ids2_tensor = torch.tensor(tokens_ids2)\n",
    "    attn_mask2 = (tokens_ids2_tensor != 1).long() # [PAD] => 1\n",
    "\n",
    "    print(ix, len(tokens_ids2))\n",
    "\n",
    "    cont2 = bert_model(tokens_ids2_tensor.unsqueeze(0), attention_mask=attn_mask2.unsqueeze(0))\n",
    "\n",
    "    for i, token in enumerate(tokens2):\n",
    "      embd = cont2.last_hidden_state[0][i].detach().numpy()\n",
    "      token_embd_per_doc.append(embd)\n",
    "\n",
    "    token_embd[label] = (tokens, token_embd_per_doc)\n",
    "    \n",
    "  else:\n",
    "    tokens_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "    attn_mask = (tokens_ids_tensor != 1).long() # [PAD] => 1\n",
    "\n",
    "    print(ix, len(tokens_ids))\n",
    "\n",
    "    cont = bert_model(tokens_ids_tensor.unsqueeze(0), attention_mask=attn_mask.unsqueeze(0))\n",
    "\n",
    "\n",
    "    token_embd_per_doc = []\n",
    "    for i, token in enumerate(tokens):\n",
    "      embd = cont.last_hidden_state[0][i].detach().numpy()\n",
    "      token_embd_per_doc.append(embd)\n",
    "\n",
    "    token_embd[label] = (tokens, token_embd_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28fbba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token_embd, open(\"token_embd.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb741f4a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4159a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embd = pickle.load(open(\"token_embd.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7181031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 609 ms\n",
      "Wall time: 617 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_token_embd = {}\n",
    "for label in token_embd.keys():\n",
    "    ix = df[df[\"label\"] == label].index[0]\n",
    "    preprocess = eval(df.loc[ix][\"preprocess\"])\n",
    "    doc_clean = preprocess[\"doc_clean\"]\n",
    "    matching = preprocess[\"matching\"]\n",
    "    uni_stop = max([i for i, v in enumerate(matching) if type(v[0]) == int])+1\n",
    "    \n",
    "    doc_clean = doc_clean[:uni_stop]\n",
    "    doc_clean = [ x.replace(\"\\ue4f8\", \"\") for x in doc_clean ]\n",
    "    \n",
    "    tokens, token_embd_per_doc = token_embd[label]\n",
    "\n",
    "    new_tokens = []\n",
    "    new_token_embd_per_doc = []\n",
    "    \n",
    "    j = 0\n",
    "    token_j = doc_clean[j]\n",
    "    token_k = \"\"\n",
    "    token_embd_k = np.zeros(768)\n",
    "    lenght_k = 0\n",
    "    for i, token_i in enumerate(tokens):\n",
    "        token_embd_i = token_embd_per_doc[i]\n",
    "        token_i = token_i if token_i[:2] != \"##\" else token_i[2:]\n",
    "        \n",
    "        token_k += token_i\n",
    "        token_embd_k += token_embd_i\n",
    "        lenght_k += 1\n",
    "        if token_k == token_j:\n",
    "            j += 1\n",
    "            token_embd_k = token_embd_k/lenght_k\n",
    "            new_tokens.append(token_k)\n",
    "            new_token_embd_per_doc.append(token_embd_k)      \n",
    "            \n",
    "            token_k = \"\"\n",
    "            token_embd_k = np.zeros(768)\n",
    "            lenght_k = 0\n",
    "            try: token_j = doc_clean[j].replace(\"\\ue4f8\", \"\")\n",
    "            except: pass\n",
    "        \n",
    "    new_token_embd[label] = (new_tokens, new_token_embd_per_doc)\n",
    "#     print(ix, new_tokens == doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff4f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_embd = {}\n",
    "for label, (tokens, token_embd_per_doc) in new_token_embd.items():\n",
    "    terms = tokens\n",
    "    terms_embd_per_doc = token_embd_per_doc\n",
    "    for i in range(len(tokens)-1):\n",
    "        bi_gram = f\"{tokens[i]}_{tokens[i+1]}\"\n",
    "        bi_gram_embd = (token_embd_per_doc[i] + token_embd_per_doc[i+1]) / 2\n",
    "        terms.append(bi_gram)\n",
    "        terms_embd_per_doc.append(bi_gram_embd)\n",
    "    \n",
    "    term_embd[label] = (terms, terms_embd_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87259d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(term_embd, open(\"term_embd.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
